<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN"
  "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
<head>
<meta name="generator" content="jemdoc, see http://jemdoc.jaboc.net/" />
<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
<link rel="stylesheet" href="jemdoc.css" type="text/css" />
<title></title>
</head>
<body>
<table summary="Table for page layout." id="tlayout">
<tr valign="top">
<td id="layout-menu">
<div class="menu-category">Menu</div>
<div class="menu-item"><a href="index.html" class="current">home</a></div>
<div class="menu-item"><a href="teaching.html">teaching</a></div>
<div class="menu-item"><a href="papers.html">academic&nbsp;papers</a></div>
</td>
<td id="layout-content">
<h1>Academic Papers</h1>
<h3>Research</h3>
<h4>Deep Learning Theory </h4>
<p>Glasgow, M. SGD Finds then Tunes Features in Two-Layer Neural Networks with near-Optimal Sample Complexity: A Case Study in the XOR problem. <a href="https://arxiv.org/abs/2309.15111">Arxiv.</a> <i>ICLR 2024 (Spotlight).</i></p>
<p>Mahankali, A., Haochen, J.,Dong, K., Glasgow, M., Ma, T. Beyond NTK with Vanilla Gradient Descent: A Mean-Field Analysis of Neural Networks with Polynomial Width, Samples, and Time. <a href="https://arxiv.org/abs/2306.16361">Arxiv.</a> <i>Neurips 2023.</i></p>
<p>Glasgow, M., Wei, C., Wootters, M., Ma, T. Max-Margin Works while Large Margin Fails: Generalization without Uniform Convergence <a href="https://arxiv.org/abs/2206.07892">Arxiv.</a> <i>ICLR 2023.</i></p>
<h4>Distributed Optimization</h4>
<p>Patel, K.*,  Glasgow, M.*, Wang, L., Joshi, N., Srebro, N. On the Still Unreasonable Effectiveness of Federated Averaging for Heterogeneous Distributed Learning <a href="https://arxiv.org/abs/2405.11667">Arxiv.</a> <i>COLT 2024.</i></p>
<p>Glasgow, M.*, Yuan, H.*, Ma, T. Sharp Bounds for Federated Averaging (Local SGD) and Continous Perspective <a href="http://arxiv.org/abs/2111.03741">Arxiv.</a> <i>AISTATS 2022.</i></p>
<p>Glasgow, M., Wootters, M. Asynchronous Distributed Optimization with Randomized Delays. <a href="https://arxiv.org/abs/2009.10717">Arxiv.</a> <i>AISTATS 2022.</i></p>
<p>Glasgow, M., Wootters, M. Approximate Gradient Coding with Optimal Decoding. <a href="https://arxiv.org/abs/2006.09638">Arxiv.</a> <i>IEEE JSAIT 2021.</i></p>
<h4>Other ML Theory</h4>
<p>Glasgow, M., Rakhlin, A. Tight Bounds for gamma-Regret via the Decision-Estimation Coefficient <a href="https://arxiv.org/abs/2303.03327">Arxiv 2023.</a></p>
<p>Tamkin, A., Glasgow, M., He, X., Goodman, N. Feature Dropout: Revisiting the Role of Augmentations in Contrastive Learning <a href="https://arxiv.org/abs/2212.08378">Arxiv.</a> <i>Neurips 2023.</i></p>
<h4>Discrete Random Matrix Theory</h4>
<p>(A-Z) Glasgow, M., Kwan, M., Sah, A., Sawhney, M. A central limit theorem for the matching number of a sparse random graph. <a href="https://arxiv.org/abs/2402.05851">Arxiv 2024.</a></p>
<p>(A-Z) Glasgow, M., Kwan, M., Sah, A., Sawhney, M. The Exact Rank of Sparse Random Graphs <a href="https://arxiv.org/abs/2303.05435">Arxiv 2023.</a></p>
<p>(A-Z) DeMichele, P., Glasgow, M., Moreira, A. On the Rank, Kernel, and Core of Sparse Random Matrices <a href="https://arxiv.org/abs/2105.11718">Arxiv 2022.</a> <i>RSA 2024.</i></p>
<h4>Undergraduate Research</h4>
<p>Tomezsko, P.J., Corbin, V.D.A., Gupta, P. et al. Determination of RNA structural diversity and its role in HIV-1 RNA splicing. Nature (2020) <a href="https://www.nature.com/articles/s41586-020-2253-5">Nature</a> <br /> <a href="https://www.dropbox.com/s/ris5ps08waz0oil/Using_EM_clustering_to_study_heterogeneity_in_RNA_Secondary_Structure.pdf?dl=0">Summary of my personal contributions to this work</a>. <i>Undergraduate Research in Silvia Roukin's lab at the Whitehead Institue.</i> </p>
<p>Glasgow, M. Unexpected transitions between repeating patterns in continued fractions. <i>In preparation.</i> <a href="https://www.dropbox.com/s/09q0q29mz89kzl8/rec_quotients.pdf?dl=0">Manuscript</a> <i>Undergraduate research with Henry Cohn.</i></p>
<h3>Expository Writing</h3>
<p><a href="https://blog.simons.berkeley.edu/2021/07/trends-in-machine-learning-theory/">Trends in Machine Learning Theory</a> <i>Part of the <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">ALT 2021 Highlights Series</a>.</i></p>
<p><a href="https://differentialprivacy.org/alt-highlights/">An Equivalence between Private Learning and Online Learning</a> <i>Part of the <a href="https://www.let-all.com/blog/2021/04/20/alt-highlights-2021/">ALT 2021 Highlights Series</a>.</i> </p>
<p><a href="https://www.dropbox.com/s/toh2p9675shc0nn/Approximation_Algorithms_for_Graphic_TSP.pdf?dl=0">Approximation Algorithms for Graphic TSP</a> <i>Final Project with Bertie Ancona for Advanced Algorithms (6.854) Fall &rsquo;17.</i></p>
<p><a href="https://www.dropbox.com/s/8ftw7kvn45w2ujc/Theorems_about_Matrix_Groups_in__GL_n_mathbb_Q.pdf?dl=0">Matrix Groups in GL_n(Q)</a> <i>Final Project for Number Theory Seminar (18.784) Fall &rsquo;16.</i></p>
<h3>Other</h3>
<p><a href="https://www.dropbox.com/s/gk8l6sd7lu8ih2r/LB_L1_Projection.pdf?dl=0">A lower bound for average linear embeddings L_2^2 &ndash;&gt; L_1</a> <i>Based on some work during my rotation with Moses Charikar in Fall &rsquo;18.</i></p>
<h1></h1>
<p><i>&ldquo;A mathematician, like a painter or a poet, is a maker of patterns.
If his patterns are more permanent than theirs, it is because they
are made with ideas.&rdquo;</i> - G. H. Hardy</p>
<div id="footer">
<div id="footer-text">
Page generated 2024-05-21 08:11:04 PDT, by <a href="http://jemdoc.jaboc.net/">jemdoc</a>.
</div>
</div>
</td>
</tr>
</table>
</body>
</html>
